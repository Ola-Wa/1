# verify version
!python --version
!pip install --upgrade luxai_s2
!pip install gym==0.19
!cp -r ../input/lux-ai-season-2/* .
add Codeadd Markdown
%%writefile /opt/conda/lib/python3.7/site-packages/luxai_s2/version.py
__version__ = ""
# this code above is used for Kaggle Notebooks
# You might not need to run this but if you get an attribute error about the gym package, run it
add Codeadd Markdown
Once the above is installed, make sure to restart and clear cell outputs otherwise the code below won't run. Once that is done we can get started!

add Codeadd Markdown
from luxai_s2.env import LuxAI_S2
import matplotlib.pyplot as plt
import numpy as np
add Codeadd Markdown
from lux.kit import obs_to_game_state, GameState, EnvConfig
from luxai_s2.utils import animate
from lux.utils import direction_to, my_turn_to_place_factory
import gym
add Codeadd Markdown
import pandas as pd
add Codeadd Markdown
def animate(imgs, _return=True):
    # using cv2 to generate videos as moviepy doesn't work on kaggle notebooks
    import cv2
    import os
    import string
    import random
    video_name = ''.join(random.choice(string.ascii_letters) for i in range(18))+'.webm'
    height, width, layers = imgs[0].shape
    fourcc = cv2.VideoWriter_fourcc(*'VP90')
    video = cv2.VideoWriter(video_name, fourcc, 10, (width,height))
​
    for img in imgs:
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        video.write(img)
    video.release()
    if _return:
        from IPython.display import Video
        return Video(video_name)
def interact(env, agents, steps):
    # reset our env
    obs = env.reset()
    np.random.seed(0)
    imgs = []
    step = 0
    # Note that as the environment has two phases, we also keep track a value called 
    # `real_env_steps` in the environment state. The first phase ends once `real_env_steps` is 0 and used below
​
    # iterate until phase 1 ends
    while env.state.real_env_steps < 0:
        if step >= steps: break
        actions = {}
        for player in env.agents:
            o = obs[player]
            a = agents[player].early_setup(step, o)
            actions[player] = a
        step += 1
        obs, rewards, dones, infos = env.step(actions)
        imgs += [env.render("rgb_array", width=640, height=640)]
    done = False
    while not done:
        if step >= steps: break
        actions = {}
        for player in env.agents:
            o = obs[player]
            a = agents[player].act(step, o)
            actions[player] = a
        step += 1
        obs, rewards, dones, infos = env.step(actions)
        imgs += [env.render("rgb_array", width=640, height=640)]
        done = dones["player_0"] and dones["player_1"]
    return animate(imgs)
add Codeadd Markdown
We can now create an environment and start interacting with it, as well as look at what the observation is like

add Codeadd Markdown
env = LuxAI_S2() # create the environment object
obs = env.reset(seed=41) # resets an environment with a seed
add Codeadd Markdown
#przyklad slownika
# stolice = {'Polska':'Warszawa', 'Singapur': 'Singapur_miasto'}
​
# slownik = dict()
# slownik['Niemcy'] = 'Berlin'
# slownik['Polska'] = 'Warszawa'
​
# slownik
add Codeadd Markdown
# the observation is always composed of observations for both players.
obs.keys(), obs["player_0"].keys()
add Codeadd Markdown
env.agents
add Codeadd Markdown
env.state.env_cfg
add Codeadd Markdown
Lokalizacja startowa
add Codeadd Markdown
To visualize the environment, on jupyter notebooks you have two options

With the rgb_array mode you can visualize every step as an environment episode progresses.

With the CLI tool, you can run an episode and save a replay.json to upload to https://s2vis.lux-ai.org/ or a replay.html file to directly open and watch

add Codeadd Markdown
# visualize the environment so far with rgb_array to get a quick look at the map
# dark orange - high rubble, light orange - low rubble
# blue = ice, yellow = ore
map_rendered = env.render("rgb_array", width=1000, height=1000)
plt.imshow(map_rendered)
add Codeadd Markdown
ice = obs["player_0"]["board"]["ice"]
add Codeadd Markdown
type(ice)
add Codeadd Markdown
ice.shape
add Codeadd Markdown
plt.imshow(ice)
add Codeadd Markdown
from scipy.ndimage import distance_transform_cdt
​
def manhattan_distance(binary_mask):
    # Get the distance map from every pixel to the nearest positive pixel
    distance_map = distance_transform_cdt(binary_mask, metric='taxicab')
    return distance_map
add Codeadd Markdown
dist_ice = manhattan_distance(1-ice)
add Codeadd Markdown
dist_ice
add Codeadd Markdown
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))
im1 = ax1.imshow(map_rendered )
ax1.set_title('map')
im2 = ax2.imshow(ice)
ax2.set_title("ice")
im3 = ax3.imshow(dist_ice)
ax3.set_title("distance to the closest ice")
fig.colorbar(im3, ax=ax3)
plt.figure(dpi=150)
plt.show()
add Codeadd Markdown
valid_spawn_locations = obs["player_0"]["board"]["valid_spawns_mask"]
add Codeadd Markdown
valid_spawn_locations.shape, dist_ice.shape
add Codeadd Markdown
valid_spawn_locations
add Codeadd Markdown
dist_ice
add Codeadd Markdown
valid_spawn_locations*dist_ice
add Codeadd Markdown
dist_ice
add Codeadd Markdown
dist_ice.max()-dist_ice
add Codeadd Markdown
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))
im1 = ax1.imshow(valid_spawn_locations)
ax1.set_title('valid spawn locations')
im2 = ax2.imshow(dist_ice)
ax2.set_title("distance to ice")
start_map = valid_spawn_locations*(dist_ice.max()-dist_ice)
im3 = ax3.imshow(start_map)
ax3.set_title("valid spawn locations x distance to ice")
fig.colorbar(im3, ax=ax3)
plt.figure(dpi=150)
plt.show()
add Codeadd Markdown
start_map
add Codeadd Markdown
start_map.max()
add Codeadd Markdown
start_map==start_map.max()
add Codeadd Markdown
np.argwhere(start_map==start_map.max())[:2]
add Codeadd Markdown
start_loc_candidates = np.argwhere(start_map==start_map.max())
add Codeadd Markdown
start_loc_candidates[:5]
add Codeadd Markdown
fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(18, 5))
im1 = ax1.imshow(valid_spawn_locations)
ax1.set_title('valid spawn locations')
im2 = ax2.imshow(dist_ice)
ax2.set_title("distance to ice")
start_map = valid_spawn_locations*(dist_ice.max()-dist_ice)
im3 = ax3.imshow(start_map)
spawn_map = np.zeros((48,48))
for i in start_loc_candidates:
    x= i[0]
    y= i[1]
    spawn_map[x,y]=100
im4 = ax4.imshow(spawn_map)
​
ax4.set_title("spawn map")
fig.colorbar(im4, ax=ax4)
plt.figure(dpi=150)
plt.show()
add Codeadd Markdown
potential_spawns = np.array(list(zip(*np.where(obs['player_0']["board"]["valid_spawns_mask"] == 1))))
spawn_loc = start_loc_candidates[np.random.randint(0, len(start_loc_candidates))]
add Codeadd Markdown
rubble = obs["player_0"]["board"]["rubble"].copy()
add Codeadd Markdown
start_loc_candidates[7]
add Codeadd Markdown
sp_x, sp_y = start_loc_candidates[0]
add Codeadd Markdown
close_rubble = rubble[sp_x-2:sp_x+3,sp_y-2:sp_y+3]
add Codeadd Markdown
close_rubble
add Codeadd Markdown
close_rubble[1:4,1:4]=100
add Codeadd Markdown
close_rubble
add Codeadd Markdown
(close_rubble == 0).mean()
add Codeadd Markdown
close_rubble[]
add Codeadd Markdown
rubble = obs["player_0"]["board"]["rubble"]
spawn_candidate_scores = {}
for i, spawn_loc_candidate in enumerate(start_loc_candidates):
    sp_x, sp_y = spawn_loc_candidate
    rub_range = 2
    close_rubble_map = rubble[sp_x-rub_range:sp_x+rub_range+1, sp_y-rub_range:sp_y+rub_range+1]
    score = (close_rubble_map==0).mean()
    spawn_candidate_scores[i]=score
    #if i > 4: break
​
key_of_best_candidate = max(spawn_candidate_scores, key=spawn_candidate_scores.get)
spawn_loc =start_loc_candidates[key_of_best_candidate]
add Codeadd Markdown
close_rubble_map[1:4,1:4] = 100
close_rubble_map[0,0] = 100
close_rubble_map[0,4] = 100
close_rubble_map[4,0] = 100
close_rubble_map[4,4] = 100
add Codeadd Markdown
close_rubble_map 
add Codeadd Markdown
spawn_loc
add Codeadd Markdown
from scipy.ndimage import distance_transform_cdt
​
def manhattan_distance(binary_mask):
    # Get the distance map from every pixel to the nearest positive pixel
    distance_map = distance_transform_cdt(binary_mask, metric='taxicab')
    return distance_map
​
valid_spawn_locations = obs["player_0"]["board"]["valid_spawns_mask"]
ice = obs["player_0"]["board"]["ice"]
dist_ice = manhattan_distance(1-ice)
start_map = valid_spawn_locations*(dist_ice.max()-dist_ice)
start_loc_candidates = np.argwhere(start_map==start_map.max())
spawn_loc = start_loc_candidates[np.random.randint(0, len(start_loc_candidates))]
add Codeadd Markdown
Wprowadzenie do klas
add Codeadd Markdown
# create a class
class Room:
    length = 0.0
    breadth = 0.0
    def calculate_area(self): #a method to calculate area
        print("Area of Room =", self.length * self.breadth)
​
# create object of Room class
study_room = Room()
add Codeadd Markdown
# assign values to all the attributes 
study_room.length = 5
study_room.breadth = 5
​
# access method inside class
study_room.calculate_area()
add Codeadd Markdown
Building an Agent
Now we know what the environment looks like, let's try building a working agent. The goal of this environment to ensure at least one factory stays alive by the end of the episode and grow as much lichen as possible.

In our kit we provide a skeleton for building an agent. Avoid removing any function from the kit unless you know what you are doing as it may cause your agent to fail on the competition servers. This agent defintion should be stored in the agent.py file.

The agent will have self.player, self.opp_player, self.env_cfg populated with the correct values at each step of an environment during competition or when you use the CLI tool to run matches.

self.env_cfg stores the curent environment's configurations, and self.player, self.opp_player stores the name of your player/team and the opposition respectively (will always be "player_0" or "player_1").

add Codeadd Markdown
class Agent():
    def __init__(self, player: str, env_cfg: EnvConfig) -> None:
        self.player = player
        self.opp_player = "player_1" if self.player == "player_0" else "player_0"
        np.random.seed(0)
        self.env_cfg: EnvConfig = env_cfg
​
    def early_setup(self, step: int, obs, remainingOverageTime: int = 60):
        actions = dict()
        # optionally convert observations to python objects with utility functions
        game_state = obs_to_game_state(step, self.env_cfg, obs) 
        return actions
​
    def act(self, step: int, obs, remainingOverageTime: int = 60):
        actions = dict()
        game_state = obs_to_game_state(step, self.env_cfg, obs)
        return actions
add Codeadd Markdown
Note that in season 2, there are two distinct phases of the game where you will have to program two different sets of logic to play it.

We will also define a simple function to initialize our agent and interact with the environment and generate a simple video replay. No need to worry about how this works specifically, you can copy paste this as you see fit. Note that this is a simplified representation. In order to visually see all numbers and details about units, factories etc. use our web visualizer.

add Codeadd Markdown
Early Phase
During the first turn of the game, each player is given the map, starting resources (N factories and N*150 water and ore), and are asked to bid for who goes first/second. Each 1 bid removes 1 water and 1 ore from that player's starting resources. Each player responds in turn 1 with their bid, which can be positive to prefer going first or negative to prefer going second.

After bidding you then place each of your factories. Each team gets N factories to place, with. For conveniency the observation contains all possible spawn locations for your team which account for the opponent's factories and resource tiles which you can't spawn on.

We will write a simple early_setup function to return the appropriate action to handle this phase.

add Codeadd Markdown
dict(faction="AlphaStrike", bid=0)
add Codeadd Markdown
np.random.randint(0, 10)
add Codeadd Markdown
def early_setup(self, step: int, obs, remainingOverageTime: int = 60):
    if step == 0:
        # bid 0 to not waste resources bidding and declare as the default faction
        # you can bid -n to prefer going second or n to prefer going first in placement
        return dict(faction="AlphaStrike", bid=0)
    else:
        game_state = obs_to_game_state(step, self.env_cfg, obs)
        # factory placement period                
        # how many factories you have left to place
        factories_to_place = game_state.teams[self.player].factories_to_place
        # whether it is your turn to place a factory
        my_turn_to_place = my_turn_to_place_factory(game_state.teams[self.player].place_first, step)
        if factories_to_place > 0 and my_turn_to_place:
            # we will spawn our factory in a random location with 150 metal and water if it is our turn to place
            potential_spawns = np.array(list(zip(*np.where(obs["board"]["valid_spawns_mask"] == 1))))
            spawn_loc = potential_spawns[np.random.randint(0, len(potential_spawns))]
            return dict(spawn=spawn_loc, metal=150, water=150)
        
        return dict()
Agent.early_setup = early_setup
add Codeadd Markdown
Each team gets N factories to place, with. For conveniency the observation contains all possible spawn locations for your team

add Codeadd Markdown
# recreate our agents and run
agents = {player: Agent(player, env.state.env_cfg) for player in env.agents}
interact(env, agents, 10)
add Codeadd Markdown
Congratz, we have lift off! We got both teams to spawn some factories. With the early phase over, we can now start programming the logic to power the next phase.

Regular Phase
The goal of the game is to grow more lichen than your opponent by the end of the 1000 step episode (not including early phase steps). To grow lichen factories must consume water. Moreover, factories passively consume 1 water a turn, and you must ensure at least one factory survives until the end. Whichever team loses all their factories first will automatically lose.

To obtain water, robots/units must mine ice from ice tiles (blue on the map) and deliver them back to a factory which then automatically refines ice into water.

Moreover, to help us write better code, we will use the provided function obs_to_game_state to convert observations from raw dictionaries to interactable python objects. Finally, we will also use a provided animate function to easily generate a simple video of the episode and embed it into here without having to upload a replay file to the web visualizer.

add Codeadd Markdown
Building Robots
Only factories can build robots, so for each factory on our team, if there is enough metal and power, we will issue a command to build a new heavy robot.

More advanced strategies will be able to efficiently leverage light units as well to collect resources but for simplicity, this tutorial uses heavy robots since they don't need to move as often to collect many resources but can collect many at a time.

add Codeadd Markdown
#from lux.kit import obs_to_game_state, GameState
add Codeadd Markdown
slownik.items()
add Codeadd Markdown
for unit_id, factory in slownik.items():
    print(unit_id, factory)
add Codeadd Markdown
env.state.env_cfg.ROBOTS["HEAVY"].POWER_COST
add Codeadd Markdown
env.state.env_cfg.ROBOTS["HEAVY"].METAL_COST
add Codeadd Markdown
zespoly =[['Pawel','Janek', 'Ola'],
['Julek', 'MAciek','Anna']]
add Codeadd Markdown
for a,b,c in zespoly:
    print(a,b,c)
add Codeadd Markdown
def act(self, step: int, obs, remainingOverageTime: int = 60):
    actions = dict()
    game_state: GameState = obs_to_game_state(step, self.env_cfg, obs)
    factories = game_state.factories[self.player]
    for unit_id, factory in factories.items():
        if factory.power >= self.env_cfg.ROBOTS["HEAVY"].POWER_COST and \
        factory.cargo.metal >= self.env_cfg.ROBOTS["HEAVY"].METAL_COST:
            actions[unit_id] = factory.build_heavy()
    return actions
Agent.act = act
add Codeadd Markdown
# recreate our agents and run
agents = {player: Agent(player, env.state.env_cfg) for player in env.agents}
interact(env, agents, 25)
add Codeadd Markdown
Robots built, but they're idle! Let's get them to work.

Moving Robots and Mining
We want the robots to find the closest ice tile and mine it. We'll worry about delivering the ice back home later. Let's update our act function to add this functionality. We will iterate over all our units and find the closest ice tile. Then we will use a given utility function that gives us the direction that takes us towards the desired ice tile. Lastly, we check if we have enough power to move and can move in that direction and submit that action.

Importantly, in this season units are given action queues (a list of actions). Each time an action queue is given, the unit's old action queue is replaced completely in addition to a small additional action queue submission power cost.

Moreover, each action has two other attributes, its execution count n and repeat. Each time the action is succesfully executed, we decrement n by 1. If n hits 0, we remove the action from the action queue. If repeat == 0, then we don't recycle the action. If repeat > 0, then we recycle the same action to the back of the action queue but this time with n = repeat.

For this tutorial, we will be giving units one action at a time with no action recycling or multiple executions. For more advanced competitors, to reduce power costs you may want to submit longer action queues and only update the action queue every once in a while.

add Codeadd Markdown
from lux.utils import direction_to
import sys
def act(self, step: int, obs, remainingOverageTime: int = 60):
    actions = dict()
    game_state = obs_to_game_state(step, self.env_cfg, obs)
    factories = game_state.factories[self.player]
    for unit_id, factory in factories.items():
        if factory.power >= self.env_cfg.ROBOTS["HEAVY"].POWER_COST and \
        factory.cargo.metal >= self.env_cfg.ROBOTS["HEAVY"].METAL_COST:
            actions[unit_id] = factory.build_heavy()
            
    # iterate over our units and have them mine the closest ice tile
    units = game_state.units[self.player]
    ice_map = game_state.board.ice # flip the board as it stores by rows then columns
    ice_tile_locations = np.argwhere(ice_map == 1) # numpy magic to get the position of every ice tile
    for unit_id, unit in units.items():
        # compute the distance to each ice tile from this unit and pick the closest
        ice_tile_distances = np.mean((ice_tile_locations - unit.pos) ** 2, 1)
        closest_ice_tile = ice_tile_locations[np.argmin(ice_tile_distances)]
        
        # if we have reached the ice tile, start mining if possible
        if np.all(closest_ice_tile == unit.pos):
            if unit.power >= unit.dig_cost(game_state) + unit.action_queue_cost(game_state):
                actions[unit_id] = [unit.dig(repeat=0, n=1)]
        else:
            direction = direction_to(unit.pos, closest_ice_tile)
            move_cost = unit.move_cost(game_state, direction)
            # check move_cost is not None, meaning that direction is not off the map or blocked
            # check if unit has enough power to move in addition to updating the action queue.
            if move_cost is not None and unit.power >= move_cost + unit.action_queue_cost(game_state):
                actions[unit_id] = [unit.move(direction, repeat=0, n=1)]
        # since we are using the simple embedded visualizer, we will have to print out details about units
        # importantly, note that we print with file=sys.stderr. Printing with anything will cause your agent to fail
        if unit.cargo.ice > 50:
            print(game_state.real_env_steps, unit, f"has {unit.cargo.ice} ice", file=sys.stderr)
    return actions
Agent.act = act
add Codeadd Markdown
# recreate our agents and run
agents = {player: Agent(player, env.state.env_cfg) for player in env.agents}
interact(env, agents, steps=40)
add Codeadd Markdown
And they're off! The heavy robots have started to move towards the ice tiles and some have begun mining.

Delivering Resources and Keep Factories Alive
We now have ice being mined, but we now need to deliver that back to the factories so they can refine that ice into water and sustain themselves.

add Codeadd Markdown
def act(self, step: int, obs, remainingOverageTime: int = 60):
    actions = dict()
    game_state = obs_to_game_state(step, self.env_cfg, obs)
    factories = game_state.factories[self.player]
    factory_tiles, factory_units = [], []
    for unit_id, factory in factories.items():
        if factory.power >= self.env_cfg.ROBOTS["HEAVY"].POWER_COST and \
        factory.cargo.metal >= self.env_cfg.ROBOTS["HEAVY"].METAL_COST:
            actions[unit_id] = factory.build_heavy()
        factory_tiles += [factory.pos]
        factory_units += [factory]
    factory_tiles = np.array(factory_tiles)
​
    units = game_state.units[self.player]
    ice_map = game_state.board.ice
    ice_tile_locations = np.argwhere(ice_map == 1)
    for unit_id, unit in units.items():
        
        # track the closest factory
        closest_factory = None
        adjacent_to_factory = False
        if len(factory_tiles) > 0:
            factory_distances = np.mean((factory_tiles - unit.pos) ** 2, 1)
            closest_factory_tile = factory_tiles[np.argmin(factory_distances)]
            closest_factory = factory_units[np.argmin(factory_distances)]
            adjacent_to_factory = np.mean((closest_factory_tile - unit.pos) ** 2) == 0
        
            # previous ice mining code
            if unit.cargo.ice < 40:
                ice_tile_distances = np.mean((ice_tile_locations - unit.pos) ** 2, 1)
                closest_ice_tile = ice_tile_locations[np.argmin(ice_tile_distances)]
                if np.all(closest_ice_tile == unit.pos):
                    if unit.power >= unit.dig_cost(game_state) + unit.action_queue_cost(game_state):
                        actions[unit_id] = [unit.dig(repeat=0, n=1)]
                else:
                    direction = direction_to(unit.pos, closest_ice_tile)
                    move_cost = unit.move_cost(game_state, direction)
                    if move_cost is not None and unit.power >= move_cost + unit.action_queue_cost(game_state):
                        actions[unit_id] = [unit.move(direction, repeat=0, n=1)]
            # else if we have enough ice, we go back to the factory and dump it.
            elif unit.cargo.ice >= 40:
                direction = direction_to(unit.pos, closest_factory_tile)
                if adjacent_to_factory:
                    if unit.power >= unit.action_queue_cost(game_state):
                        actions[unit_id] = [unit.transfer(direction, 0, unit.cargo.ice, repeat=0, n=1)]
                else:
                    move_cost = unit.move_cost(game_state, direction)
                    if move_cost is not None and unit.power >= move_cost + unit.action_queue_cost(game_state):
                        actions[unit_id] = [unit.move(direction, repeat=0, n=1)]
    return actions
Agent.act = act
add Codeadd Markdown
# recreate our agents and run
agents = {player: Agent(player, env.state.env_cfg) for player in env.agents}
interact(env, agents, 300)
add Codeadd Markdown
Some factories are surviving for for more than 150 steps thanks to the delivery of additional ice, but more work will need to be done to keep them alive longer.

Puting all those pieces together the full starter agent looks like this (and we will save it to agent.py)

add Codeadd Markdown
Before hunting (Macro)
add Codeadd Markdown
%%writefile agent.py
from lux.kit import obs_to_game_state, GameState, EnvConfig
from lux.utils import direction_to, my_turn_to_place_factory
import numpy as np
import sys
from scipy.ndimage import distance_transform_cdt
​
def manhattan_distance(binary_mask):
    distance_map = distance_transform_cdt(binary_mask, metric='taxicab')
    return distance_map
​
class Agent():
    def __init__(self, player: str, env_cfg: EnvConfig) -> None:
        self.player = player
        self.opp_player = "player_1" if self.player == "player_0" else "player_0"
        np.random.seed(0)
        self.env_cfg: EnvConfig = env_cfg
​
    def early_setup(self, step: int, obs, remainingOverageTime: int = 60):
        if step == 0:
            # bid 0 to not waste resources bidding and declare as the default faction
            return dict(faction="AlphaStrike", bid=2)
        else:
            game_state = obs_to_game_state(step, self.env_cfg, obs)
            # factory placement period
​
            # how much water and metal you have in your starting pool to give to new factories
            water_left = game_state.teams[self.player].water
            metal_left = game_state.teams[self.player].metal
​
            # how many factories you have left to place
            factories_to_place = game_state.teams[self.player].factories_to_place
            # whether it is your turn to place a factory
            my_turn_to_place = my_turn_to_place_factory(game_state.teams[self.player].place_first, step)
            if factories_to_place > 0 and my_turn_to_place:
                # we will spawn our factory in a random location with 150 metal and water if it is our turn to place
#                 potential_spawns = np.array(list(zip(*np.where(obs["board"]["valid_spawns_mask"] == 1))))
#                 spawn_loc = potential_spawns[np.random.randint(0, len(potential_spawns))]
#                 return dict(spawn=spawn_loc, metal=150, water=150)
            
                valid_spawn_locations = obs["board"]["valid_spawns_mask"]
                ice = obs["board"]["ice"]
                dist_ice = manhattan_distance(1-ice)
                start_map = valid_spawn_locations*(dist_ice.max()-dist_ice)
                start_loc_candidates = np.argwhere(start_map==start_map.max())
#                 spawn_loc = start_loc_candidates[np.random.randint(0, len(start_loc_candidates))]
                
                rubble = obs["board"]["rubble"]
                rub_range = 2
                spawn_candidate_scores = {}
                for i, spawn_loc_candidate in enumerate(start_loc_candidates):
                    sp_x, sp_y = spawn_loc_candidate
                    close_rubble_map = rubble[sp_x-rub_range:sp_x+rub_range+1, sp_y-rub_range:sp_y+rub_range+1]                    
                    try:
                        close_rubble_map[1:4,1:4] = 100
                        close_rubble_map[0,0] = 100
                        close_rubble_map[0,4] = 100
                        close_rubble_map[4,0] = 100
                        close_rubble_map[4,4] = 100
                    except:
                        pass
                    score = (close_rubble_map==0).mean()
                    spawn_candidate_scores[i]=score
​
                key_of_best_candidate = max(spawn_candidate_scores, key=spawn_candidate_scores.get)
                spawn_loc =start_loc_candidates[key_of_best_candidate]
​
                return dict(spawn=spawn_loc, metal=150, water=150)
​
            return dict()
​
    def act(self, step: int, obs, remainingOverageTime: int = 60):
        actions = dict()
        game_state = obs_to_game_state(step, self.env_cfg, obs)
        factories = game_state.factories[self.player]
        game_state.teams[self.player].place_first
        factory_tiles, factory_units = [], []
        for unit_id, factory in factories.items():
            if factory.power >= self.env_cfg.ROBOTS["HEAVY"].POWER_COST and \
            factory.cargo.metal >= self.env_cfg.ROBOTS["HEAVY"].METAL_COST:
                actions[unit_id] = factory.build_heavy()
            if (self.env_cfg.max_episode_length - game_state.real_env_steps < 250) and (factory.cargo.water > 50):
                if factory.water_cost(game_state) <= factory.cargo.water:
                    actions[unit_id] = factory.water()
            factory_tiles += [factory.pos]
            factory_units += [factory]
        factory_tiles = np.array(factory_tiles)
​
        units = game_state.units[self.player]
        ice_map = game_state.board.ice
        ice_tile_locations = np.argwhere(ice_map == 1)
        for unit_id, unit in units.items():
            #basic stats
            battery_capacity = 150 if unit.unit_type == "LIGHT" else 3000
            # track the closest factory
            closest_factory = None
            adjacent_to_factory = False
            if len(factory_tiles) > 0:
                factory_distances = np.mean((factory_tiles - unit.pos) ** 2, 1)
                closest_factory_tile = factory_tiles[np.argmin(factory_distances)]
                closest_factory = factory_units[np.argmin(factory_distances)]
                adjacent_to_factory = np.mean((closest_factory_tile - unit.pos) ** 2) == 0
​
                # previous ice mining code
                if (unit.power < battery_capacity*0.1) & adjacent_to_factory:
                    actions[unit_id] = [unit.pickup(4, battery_capacity-unit.power)]
                elif unit.cargo.ice < 100:
                    ice_tile_distances = np.mean((ice_tile_locations - unit.pos) ** 2, 1)
                    closest_ice_tile = ice_tile_locations[np.argmin(ice_tile_distances)]
                    if np.all(closest_ice_tile == unit.pos):
                        if unit.power >= unit.dig_cost(game_state) + unit.action_queue_cost(game_state):
                            actions[unit_id] = [unit.dig(repeat=0, n=1)]
                    else:
                        direction = direction_to(unit.pos, closest_ice_tile)
                        move_cost = unit.move_cost(game_state, direction)
                        if move_cost is not None and unit.power >= move_cost + unit.action_queue_cost(game_state):
                            actions[unit_id] = [unit.move(direction, repeat=0, n=1)]
                # else if we have enough ice, we go back to the factory and dump it.
                elif unit.cargo.ice >= 100:
                    direction = direction_to(unit.pos, closest_factory_tile)
                    if adjacent_to_factory:
                        if unit.power >= unit.action_queue_cost(game_state):
                            actions[unit_id] = [unit.transfer(direction, 0, unit.cargo.ice, repeat=0, n=1)]
                    else:
                        move_cost = unit.move_cost(game_state, direction)
                        if move_cost is not None and unit.power >= move_cost + unit.action_queue_cost(game_state):
                            actions[unit_id] = [unit.move(direction, repeat=0, n=1)]
        return actions        
add Codeadd Markdown
# Praca domowa 20.03.2023
# - Wyslac powyzszego bota na leaderboard
add Codeadd Markdown
With hunting
add Codeadd Markdown
%%writefile agent.py
from lux.kit import obs_to_game_state, GameState, EnvConfig
from lux.utils import direction_to, my_turn_to_place_factory
import numpy as np
import pandas as pd
import sys
from scipy.ndimage import distance_transform_cdt
​
def manhattan_distance(binary_mask):
    distance_map = distance_transform_cdt(binary_mask, metric='taxicab')
    return distance_map
​
# direction (0 = center, 1 = up, 2 = right, 3 = down, 4 = left)
move_arrays = {1: np.array([0, -1]), 2: np.array([1, 0]), 3: np.array([0, 1]), 4: np.array([-1, 0])}
def direction_to_mod_2(src, target, opp_factories_map, step, step_thr = 20):
    move_signs_hor = {1: 2, -1: 4}
    move_signs_vert = {1: 3, -1: 1}
​
    move_dict_vert = {1:-1, 3:1}
    move_dict_hor = {2:1, 4:-1}
    ds = target - src
    dx = ds[0]
    dy = ds[1]
 
    if dx == 0 and dy == 0:
        move_candidate = 0
        pos_after = src
    if dx > 0:
        move_candidate_x = 2
        pos_after_x = src + np.array([1,0])
    elif dx < 0:
        move_candidate_x = 4
        pos_after_x= src + np.array([-1,0])
    else:
        move_candidate_x = 0
        pos_after_x = src 
    if dy > 0:
        move_candidate_y = 3
        pos_after_y = src + np.array([0,1])
    elif dy < 0:
        move_candidate_y = 1
        pos_after_y = src + np.array([0,-1])
    else:
        move_candidate_y = 0
        pos_after_y = src
    
    move_candidates = []
    pos_after_x = (pos_after_x[0], pos_after_x[1])
    pos_after_y = (pos_after_y[0], pos_after_y[1])
    rejected_moves=-1
    if pos_after_x not in opp_factories_map:
        move_candidates.append(move_candidate_x)
    else:
        rejected_moves=move_candidate_x
    if pos_after_y not in opp_factories_map:
        move_candidates.append(move_candidate_y)
    else:
        rejected_moves=move_candidate_y
    
    if dx == 0 and dy == 0:
        move = 0
    else:
        move_candidates = [mc for mc in move_candidates if mc != 0]
        if len(move_candidates)>0:
            move = np.random.choice(move_candidates)
        else:
            if rejected_moves in [1,3]:
                for i in [1,-1,2,-2,3,-3]:
                    pos_after_move = src + np.array([i, move_dict_vert[rejected_moves]])
                    pos_after_move = (pos_after_move[0], pos_after_move[1])
                    if (pos_after_move not in opp_factories_map) and pos_after_move[0]>=0 and pos_after_move[1]>=0:
                        move_dir = np.sign(i)
                        move_times = abs(i)
                        move = [move_signs_hor[move_dir]]*move_times + [rejected_moves]
                        break
            elif rejected_moves in [2,4]:
                for i in [1,-1,2,-2,3,-3]:
                    pos_after_move = src + np.array([move_dict_hor[rejected_moves],i])
                    pos_after_move = (pos_after_move[0], pos_after_move[1])
                    if (pos_after_move not in opp_factories_map) and 0<=pos_after_move[0]<=47 and 0<=pos_after_move[1]<=47:
                        move_dir = np.sign(i)
                        move_times = abs(i)
                        move = [move_signs_vert[move_dir]]*move_times + [rejected_moves]
                        break
    return move
​
def simple_manh_distance(src, target):
    ds=src-target
    return abs(ds[0])+abs(ds[1])
​
def square3x3(pos):
    f_x, f_y = pos[0], pos[1]
    map3x3 = []
    for x in range(f_x-1,f_x+2):
        for y in range(f_y-1,f_y+2):
            map3x3.append(np.array([x,y]))
    return map3x3
​
class Agent():
    def __init__(self, player: str, env_cfg: EnvConfig) -> None:
        self.player = player
        self.opp_player = "player_1" if self.player == "player_0" else "player_0"
        np.random.seed(0)
        self.env_cfg: EnvConfig = env_cfg
            
        self.my_robots_move_queue = {}
        self.my_robots_parent_factory = {}
        self.hunting_dict = {}
        self.opp_robots = {}
    
    def early_setup(self, step: int, obs, remainingOverageTime: int = 60):
        if step == 0:
            # bid 0 to not waste resources bidding and declare as the default faction
            return dict(faction="AlphaStrike", bid=0)
        else:
            game_state = obs_to_game_state(step, self.env_cfg, obs)
            # factory placement period
​
            # how much water and metal you have in your starting pool to give to new factories
            water_left = game_state.teams[self.player].water
            metal_left = game_state.teams[self.player].metal
​
            # how many factories you have left to place
            factories_to_place = game_state.teams[self.player].factories_to_place
            # whether it is your turn to place a factory
            my_turn_to_place = my_turn_to_place_factory(game_state.teams[self.player].place_first, step)
            if factories_to_place > 0 and my_turn_to_place:
                # we will spawn our factory in a random location with 150 metal and water if it is our turn to place
#                 potential_spawns = np.array(list(zip(*np.where(obs["board"]["valid_spawns_mask"] == 1))))
#                 spawn_loc = potential_spawns[np.random.randint(0, len(potential_spawns))]
#                 return dict(spawn=spawn_loc, metal=150, water=150)
            
                valid_spawn_locations = obs["board"]["valid_spawns_mask"]
                ice = obs["board"]["ice"]
                dist_ice = manhattan_distance(1-ice)
                start_map = valid_spawn_locations*(dist_ice.max()-dist_ice)
                start_loc_candidates = np.argwhere(start_map==start_map.max())
#                 spawn_loc = start_loc_candidates[np.random.randint(0, len(start_loc_candidates))]
                
                rubble = obs["board"]["rubble"]
                spawn_candidate_scores = {}
                for i, spawn_loc_candidate in enumerate(start_loc_candidates):
                    sp_x, sp_y = spawn_loc_candidate
                    rub_range = 2
                    close_rubble_map = rubble[sp_x-rub_range:sp_x+rub_range+1, sp_y-rub_range:sp_y+rub_range+1]
                    score = (close_rubble_map==0).mean()
                    spawn_candidate_scores[i]=score
​
                key_of_best_candidate = max(spawn_candidate_scores, key=spawn_candidate_scores.get)
                spawn_loc =start_loc_candidates[key_of_best_candidate]
                return dict(spawn=spawn_loc, metal=150, water=150)
​
            return dict()
​
    def act(self, step: int, obs, remainingOverageTime: int = 60):
        #print(self.player, step)
        actions = dict()
        game_state = obs_to_game_state(step, self.env_cfg, obs)
        factories = game_state.factories[self.player]
        opp_factories = game_state.factories[self.opp_player]
        opp_factories_map = []
        for unit_id, factory in opp_factories.items():
            f_x, f_y = factory.pos
            for x in range(f_x-1,f_x+2):
                for y in range(f_y-1,f_y+2):
                    opp_factories_map.append((x,y))
        game_state.teams[self.player].place_first
        factory_tiles, factory_units = [], []
        #
        for unit_id, unit in game_state.units[self.opp_player].items():
            if unit_id not in self.opp_robots.keys():
                self.opp_robots[unit_id] = {'pos': 0, 'sit_counter':0, 'target': False, 'alive': True, 'type': ''}  
            prev_pos = self.opp_robots[unit_id]['pos']
            cur_pos = unit.pos
            if (prev_pos==cur_pos).mean()==1:
                self.opp_robots[unit_id]['sit_counter']+=1
            else:
                self.opp_robots[unit_id]['sit_counter']=0
            
            if self.opp_robots[unit_id]['sit_counter'] >= 3:
                self.opp_robots[unit_id]['target'] = True
            self.opp_robots[unit_id]['pos'] = cur_pos
            self.opp_robots[unit_id]['type'] = unit.unit_type
​
        for unit_id, factory in factories.items():
            if factory.power >= self.env_cfg.ROBOTS["HEAVY"].POWER_COST and \
            factory.cargo.metal >= self.env_cfg.ROBOTS["HEAVY"].METAL_COST:
                actions[unit_id] = factory.build_heavy()
            if (self.env_cfg.max_episode_length - game_state.real_env_steps < 250) and (factory.cargo.water > 50):
                if factory.water_cost(game_state) <= factory.cargo.water:
                    actions[unit_id] = factory.water()
            factory_tiles+=square3x3(factory.pos) 
            factory_units += [factory]
        factory_tiles = np.array(factory_tiles)
​
        units = game_state.units[self.player]
        for u_id, u in units.items():
            if u_id not in self.my_robots_parent_factory.keys():
                self.my_robots_parent_factory[u_id] = square3x3(u.pos)
​
        regular_units = {key:value for (key,value) in units.items() if key not in self.hunting_dict.keys()}
        ice_map = game_state.board.ice
        ice_tile_locations = np.argwhere(ice_map == 1)
        for unit_id, unit in regular_units.items():
            #basic stats
            battery_capacity = 150 if unit.unit_type == "LIGHT" else 3000
            # track the closest factory
            closest_factory = None
            adjacent_to_factory = False
            if len(factory_tiles) > 0:
                factory_tiles = self.my_robots_parent_factory[unit_id]
                factory_distances = np.mean((factory_tiles - unit.pos) ** 2, 1)
                closest_factory_tile = factory_tiles[np.argmin(factory_distances)]
                #closest_factory = factory_units[np.argmin(factory_distances)]
                adjacent_to_factory = np.mean((closest_factory_tile - unit.pos) ** 2) == 0
​
                # previous ice mining code
                if (unit.power < battery_capacity*0.1) & adjacent_to_factory:
                    actions[unit_id] = [unit.pickup(4, battery_capacity-unit.power)]
                elif unit.cargo.ice < 100:
                    ice_tile_distances = np.mean((ice_tile_locations - unit.pos) ** 2, 1)
                    closest_ice_tile = ice_tile_locations[np.argmin(ice_tile_distances)]
                    if np.all(closest_ice_tile == unit.pos):
                        if unit.power >= unit.dig_cost(game_state) + unit.action_queue_cost(game_state):
                            actions[unit_id] = [unit.dig(repeat=1, n=1)]
                    else:
                        direction = direction_to(unit.pos, closest_ice_tile)
                        move_cost = unit.move_cost(game_state, direction)
                        if move_cost is not None and unit.power >= move_cost + unit.action_queue_cost(game_state):
                            actions[unit_id] = [unit.move(direction, repeat=0, n=1)]
                # else if we have enough ice, we go back to the factory and dump it.
                elif unit.cargo.ice >= 178:
                    direction = direction_to(unit.pos, closest_factory_tile)
                    if adjacent_to_factory:
                        if unit.power >= unit.action_queue_cost(game_state):
                            actions[unit_id] = [unit.transfer(direction, 0, unit.cargo.ice, repeat=0, n=1)]
                    else:
                        move_cost = unit.move_cost(game_state, direction)
                        if move_cost is not None and unit.power >= move_cost + unit.action_queue_cost(game_state):
                            actions[unit_id] = [unit.move(direction, repeat=0, n=1)]
        #targets
        if True:
#         if self.player == 'player_1':
            clean_list_h = []
            clean_list_t = []            
            for h,t in self.hunting_dict.items():
                if t not in game_state.units[self.opp_player].keys():
                    clean_list_h.append(h)
                    clean_list_t.append(t)
            for t in clean_list_t:
                try:
                    self.opp_robots.pop(t)
                except:
                    print('DEBUG - unhandled error')
            for h in clean_list_h:
                self.hunting_dict.pop(h)
            def simple_manh_distance(src, target):
                ds=src-target
                return abs(ds[0])+abs(ds[1])
            
            hunting_pair_candidates = []
            for opp_unit_id, opp_unit in self.opp_robots.items():
                min_dist_to_target = 10000
                closest_unit_id = ''
                for unit_id, unit in units.items():
                    if (opp_unit['target'] == True) and (opp_unit['type'] == 'HEAVY') and (unit_id not in self.hunting_dict.keys()) and (opp_unit_id not in self.hunting_dict.values()):
                        distance = simple_manh_distance(unit.pos, opp_unit['pos'])
                        if distance < min_dist_to_target:
                            min_dist_to_target = distance
                            closest_unit_id = unit_id
                if min_dist_to_target<=15 and (units[closest_unit_id].power > 2000) :
                    hunting_pair_candidates.append({'opp_unit_id':opp_unit_id,'unit_id': closest_unit_id, 'distance': min_dist_to_target})
            
            if len(hunting_pair_candidates)>0:
                hunting_pair_candidates = pd.DataFrame(hunting_pair_candidates).sort_values('distance').drop_duplicates('unit_id', keep='first')
                for i, row in hunting_pair_candidates.iterrows():
                    self.hunting_dict[row['unit_id']]= row['opp_unit_id']
                    break
                
            dead_hunters=[]
            for hunter,target in self.hunting_dict.items():
                if hunter not in units.keys():
                    dead_hunters.append(hunter)
​
            for dh in dead_hunters:
                try:
                    self.hunting_dict.pop(dh)
                except:
                    pass
            for hunter,target in self.hunting_dict.items():        
                hunter_unit = units[hunter]
                direction = direction_to_mod_2(hunter_unit.pos, self.opp_robots[target]['pos'], opp_factories_map, step, 1000)
                if (type(direction) == list) and hunter not in self.my_robots_move_queue.keys():
                    self.my_robots_move_queue[hunter]=direction
                if hunter in self.my_robots_move_queue.keys():
                    direction = self.my_robots_move_queue[hunter][0]
                    move_cost = unit.move_cost(game_state, direction)
                    if move_cost is not None and hunter_unit.power >= move_cost + unit.action_queue_cost(game_state):
                        actions[hunter] = [hunter_unit.move(direction, repeat=0, n=1)]
                        self.my_robots_move_queue[hunter]=self.my_robots_move_queue[hunter][1:]
                        if len(self.my_robots_move_queue[hunter])==0:
                            self.my_robots_move_queue.pop(hunter)
                else:
                    move_cost = unit.move_cost(game_state, direction)
                    if move_cost is not None and hunter_unit.power >= move_cost + unit.action_queue_cost(game_state):
                        actions[hunter] = [hunter_unit.move(direction, repeat=0, n=1)]
                        #print('ah', actions[hunter])
        actions_before_coord = []
        for unit_id, unit in units.items():
            cur_pos = unit.pos
            if unit_id in actions.keys():
                cur_action = actions[unit_id]
                action_type = cur_action[0][0]
                if action_type == 0:
                    action_dir = cur_action[0][1]
                    pos_after = cur_pos + move_arrays[action_dir]                    
                else:
                    pos_after = cur_pos
            else:
                action_type = 99
                pos_after = cur_pos
            actions_before_coord.append({'unit_id':unit_id, 'action_type': action_type, 'pos_after':str(pos_after)})
        if len(actions_before_coord)>0:
            actions_before_coord = pd.DataFrame(actions_before_coord)
            actions_before_coord_wo_dupl = actions_before_coord.sort_values('action_type').drop_duplicates('pos_after', keep = 'last') 
            robots_to_wait = set(actions_before_coord['unit_id'].unique()).difference(actions_before_coord_wo_dupl['unit_id'].unique())
            for unit_id in robots_to_wait:
                actions.pop(unit_id)
        return actions
Overwriting agent.py
add Codeadd Markdown
Create a submission
Now we need to create a .tar.gz file with main.py (and agent.py) at the top level. We can then upload this!

add Codeadd Markdown
!tar -czf submission_hunting_heavy_178.tar.gz *
add Codeadd Markdown
#!tar -czf submission_unitcargo_wyborstartu_border_rubble_recharge_water750_50_bid2.tar.gz *
add Codeadd Markdown
Experiments
add Codeadd Markdown
def interact(env, agents, steps, sd):
    # reset our env
    obs = env.reset(sd)
    np.random.seed(0)
    imgs = []
    step = 0
    # Note that as the environment has two phases, we also keep track a value called 
    # `real_env_steps` in the environment state. The first phase ends once `real_env_steps` is 0 and used below
​
    # iterate until phase 1 ends
    while env.state.real_env_steps < 0:
        if step >= steps: break
        actions = {}
        for player in env.agents:
            o = obs[player]
            a = agents[player].early_setup(step, o)
            actions[player] = a
        step += 1
        obs, rewards, dones, infos = env.step(actions)
        imgs += [env.render("rgb_array", width=640, height=640)]
    done = False
    while not done:
        if step >= steps: break
        actions = {}
        for player in env.agents:
            o = obs[player]
            a = agents[player].act(step, o)
            actions[player] = a
        step += 1
        obs, rewards, dones, infos = env.step(actions)
        imgs += [env.render("rgb_array", width=640, height=640)]
        done = dones["player_0"] and dones["player_1"]
    return animate(imgs)
add Codeadd Markdown
# direction (0 = center, 1 = up, 2 = right, 3 = down, 4 = left)
def direction_to_mod(src, target, step, step_thr = 20):
    ds = target - src
    dx = ds[0]
    dy = ds[1]
    if step > step_thr:
        print('target pos', target) 
        print('src pos', src)
        print('ds dy', dx, dy) 
​
    if dx == 0 and dy == 0:
        if step > step_thr:
            print('move center')
        return 0
    if abs(dx) > abs(dy):
        if dx > 0:
            if step > step_thr:
                print('move right')
            return 2 
        else:
            if step > step_thr:
                print('move left')
            return 4
    else:
        if dy > 0:
            if step > step_thr:
                print('move down')
            return 3
        else:
            if step > step_thr:
                print('move up')
            return 1
add Codeadd Markdown
# direction (0 = center, 1 = up, 2 = right, 3 = down, 4 = left)
move_arrays = {1: np.array([0, -1]), 2: np.array([1, 0]), 3: np.array([0, 1]), 4: np.array([-1, 0])}
def direction_to_mod_2(src, target, opp_factories_map, step, step_thr = 20):
    move_signs_hor = {1: 2, -1: 4}
    move_signs_vert = {1: 3, -1: 1}
​
    move_dict_vert = {1:-1, 3:1}
    move_dict_hor = {2:1, 4:-1}
    ds = target - src
    dx = ds[0]
    dy = ds[1]
 
    if dx == 0 and dy == 0:
        move_candidate = 0
        pos_after = src
    if dx > 0:
        move_candidate_x = 2
        pos_after_x = src + np.array([1,0])
    elif dx < 0:
        move_candidate_x = 4
        pos_after_x= src + np.array([-1,0])
    else:
        move_candidate_x = 0
        pos_after_x = src 
    if dy > 0:
        move_candidate_y = 3
        pos_after_y = src + np.array([0,1])
    elif dy < 0:
        move_candidate_y = 1
        pos_after_y = src + np.array([0,-1])
    else:
        move_candidate_y = 0
        pos_after_y = src
    
    move_candidates = []
    pos_after_x = (pos_after_x[0], pos_after_x[1])
    pos_after_y = (pos_after_y[0], pos_after_y[1])
    rejected_moves=-1
    if pos_after_x not in opp_factories_map:
        move_candidates.append(move_candidate_x)
    else:
        rejected_moves=move_candidate_x
    if pos_after_y not in opp_factories_map:
        move_candidates.append(move_candidate_y)
    else:
        rejected_moves=move_candidate_y
    
    if dx == 0 and dy == 0:
        move = 0
    else:
        move_candidates = [mc for mc in move_candidates if mc != 0]
        if len(move_candidates)>0:
            move = np.random.choice(move_candidates)
        else:
            if rejected_moves in [1,3]:
                for i in [1,-1,2,-2,3,-3]:
                    pos_after_move = src + np.array([i, move_dict_vert[rejected_moves]])
                    pos_after_move = (pos_after_move[0], pos_after_move[1])
                    if (pos_after_move not in opp_factories_map) and pos_after_move[0]>=0 and pos_after_move[1]>=0:
                        move_dir = np.sign(i)
                        move_times = abs(i)
                        move = [move_signs_hor[move_dir]]*move_times + [rejected_moves]
                        break
            elif rejected_moves in [2,4]:
                for i in [1,-1,2,-2,3,-3]:
                    pos_after_move = src + np.array([move_dict_hor[rejected_moves],i])
                    pos_after_move = (pos_after_move[0], pos_after_move[1])
                    if (pos_after_move not in opp_factories_map) and 0<=pos_after_move[0]<=47 and 0<=pos_after_move[1]<=47:
                        move_dir = np.sign(i)
                        move_times = abs(i)
                        move = [move_signs_vert[move_dir]]*move_times + [rejected_moves]
                        break
    return move
add Codeadd Markdown
#move arrays
add Codeadd Markdown
def square3x3(pos):
    f_x, f_y = pos[0], pos[1]
    map3x3 = []
    for x in range(f_x-1,f_x+2):
        for y in range(f_y-1,f_y+2):
            map3x3.append(np.array([x,y]))
    return map3x3
add Codeadd Markdown
class Agent():
    def __init__(self, player: str, env_cfg: EnvConfig) -> None:
        self.player = player
        self.opp_player = "player_1" if self.player == "player_0" else "player_0"
        np.random.seed(0)
        self.env_cfg: EnvConfig = env_cfg
            
        self.my_robots_move_queue = {}
        self.my_robots_parent_factory = {}
        self.hunting_dict = {}
        self.opp_robots = {}
    
    def square3x3(pos):
        f_x, f_y = pos[0], pos[1]
        map3x3 = []
        for x in range(f_x-1,f_x+2):
            for y in range(f_y-1,f_y+2):
                map3x3.append(np.array([x,y]))
        return map3x3
​
    def early_setup(self, step: int, obs, remainingOverageTime: int = 60):
        if step == 0:
            # bid 0 to not waste resources bidding and declare as the default faction
            return dict(faction="AlphaStrike", bid=0)
        else:
            game_state = obs_to_game_state(step, self.env_cfg, obs)
            # factory placement period
​
            # how much water and metal you have in your starting pool to give to new factories
            water_left = game_state.teams[self.player].water
            metal_left = game_state.teams[self.player].metal
​
            # how many factories you have left to place
            factories_to_place = game_state.teams[self.player].factories_to_place
            # whether it is your turn to place a factory
            my_turn_to_place = my_turn_to_place_factory(game_state.teams[self.player].place_first, step)
            if factories_to_place > 0 and my_turn_to_place:
                # we will spawn our factory in a random location with 150 metal and water if it is our turn to place
#                 potential_spawns = np.array(list(zip(*np.where(obs["board"]["valid_spawns_mask"] == 1))))
#                 spawn_loc = potential_spawns[np.random.randint(0, len(potential_spawns))]
#                 return dict(spawn=spawn_loc, metal=150, water=150)
            
                valid_spawn_locations = obs["board"]["valid_spawns_mask"]
                ice = obs["board"]["ice"]
                dist_ice = manhattan_distance(1-ice)
                start_map = valid_spawn_locations*(dist_ice.max()-dist_ice)
                start_loc_candidates = np.argwhere(start_map==start_map.max())
#                 spawn_loc = start_loc_candidates[np.random.randint(0, len(start_loc_candidates))]
                
                rubble = obs["board"]["rubble"]
                spawn_candidate_scores = {}
                for i, spawn_loc_candidate in enumerate(start_loc_candidates):
                    sp_x, sp_y = spawn_loc_candidate
                    rub_range = 2
                    close_rubble_map = rubble[sp_x-rub_range:sp_x+rub_range+1, sp_y-rub_range:sp_y+rub_range+1]
                    score = (close_rubble_map==0).mean()
                    spawn_candidate_scores[i]=score
​
                key_of_best_candidate = max(spawn_candidate_scores, key=spawn_candidate_scores.get)
                spawn_loc =start_loc_candidates[key_of_best_candidate]
                return dict(spawn=spawn_loc, metal=150, water=150)
​
            return dict()
​
    def act(self, step: int, obs, remainingOverageTime: int = 60):
        #print(self.player, step)
        actions = dict()
        game_state = obs_to_game_state(step, self.env_cfg, obs)
        factories = game_state.factories[self.player]
        opp_factories = game_state.factories[self.opp_player]
        opp_factories_map = []
        for unit_id, factory in opp_factories.items():
            f_x, f_y = factory.pos
            for x in range(f_x-1,f_x+2):
                for y in range(f_y-1,f_y+2):
                    opp_factories_map.append((x,y))
        game_state.teams[self.player].place_first
        factory_tiles, factory_units = [], []
        #
        for unit_id, unit in game_state.units[self.opp_player].items():
            if unit_id not in self.opp_robots.keys():
                self.opp_robots[unit_id] = {'pos': 0, 'sit_counter':0, 'target': False, 'alive': True, 'type': ''}  
            prev_pos = self.opp_robots[unit_id]['pos']
            cur_pos = unit.pos
            if (prev_pos==cur_pos).mean()==1:
                self.opp_robots[unit_id]['sit_counter']+=1
            else:
                self.opp_robots[unit_id]['sit_counter']=0
            
            if self.opp_robots[unit_id]['sit_counter'] >= 3:
                self.opp_robots[unit_id]['target'] = True
            self.opp_robots[unit_id]['pos'] = cur_pos
            self.opp_robots[unit_id]['type'] = unit.unit_type
​
        for unit_id, factory in factories.items():
            if factory.power >= self.env_cfg.ROBOTS["HEAVY"].POWER_COST and \
            factory.cargo.metal >= self.env_cfg.ROBOTS["HEAVY"].METAL_COST:
                actions[unit_id] = factory.build_heavy()
            if (self.env_cfg.max_episode_length - game_state.real_env_steps < 500) and (factory.cargo.water > 50):
                if factory.water_cost(game_state) <= factory.cargo.water:
                    actions[unit_id] = factory.water()
            factory_tiles+=square3x3(factory.pos) 
            factory_units += [factory]
        factory_tiles = np.array(factory_tiles)
​
        units = game_state.units[self.player]
        for u_id, u in units.items():
            if u_id not in self.my_robots_parent_factory.keys():
                self.my_robots_parent_factory[u_id] = square3x3(u.pos)
​
        regular_units = {key:value for (key,value) in units.items() if key not in self.hunting_dict.keys()}
        ice_map = game_state.board.ice
        ice_tile_locations = np.argwhere(ice_map == 1)
        for unit_id, unit in regular_units.items():
            #basic stats
            battery_capacity = 150 if unit.unit_type == "LIGHT" else 3000
            # track the closest factory
            closest_factory = None
            adjacent_to_factory = False
            if len(factory_tiles) > 0:
                factory_tiles = self.my_robots_parent_factory[unit_id]
                factory_distances = np.mean((factory_tiles - unit.pos) ** 2, 1)
                closest_factory_tile = factory_tiles[np.argmin(factory_distances)]
                #closest_factory = factory_units[np.argmin(factory_distances)]
                adjacent_to_factory = np.mean((closest_factory_tile - unit.pos) ** 2) == 0
​
                # previous ice mining code
                if (unit.power < battery_capacity*0.1) & adjacent_to_factory:
                    actions[unit_id] = [unit.pickup(4, battery_capacity-unit.power)]
                elif unit.cargo.ice < 100:
                    ice_tile_distances = np.mean((ice_tile_locations - unit.pos) ** 2, 1)
                    closest_ice_tile = ice_tile_locations[np.argmin(ice_tile_distances)]
                    if np.all(closest_ice_tile == unit.pos):
                        if unit.power >= unit.dig_cost(game_state) + unit.action_queue_cost(game_state):
                            actions[unit_id] = [unit.dig(repeat=1, n=1)]
                    else:
                        direction = direction_to(unit.pos, closest_ice_tile)
                        move_cost = unit.move_cost(game_state, direction)
                        if move_cost is not None and unit.power >= move_cost + unit.action_queue_cost(game_state):
                            actions[unit_id] = [unit.move(direction, repeat=0, n=1)]
                # else if we have enough ice, we go back to the factory and dump it.
                elif unit.cargo.ice >= 100:
                    direction = direction_to(unit.pos, closest_factory_tile)
                    if adjacent_to_factory:
                        if unit.power >= unit.action_queue_cost(game_state):
                            actions[unit_id] = [unit.transfer(direction, 0, unit.cargo.ice, repeat=0, n=1)]
                    else:
                        move_cost = unit.move_cost(game_state, direction)
                        if move_cost is not None and unit.power >= move_cost + unit.action_queue_cost(game_state):
                            actions[unit_id] = [unit.move(direction, repeat=0, n=1)]
        #targets
#         if True:
        if self.player == 'player_1':
            clean_list_h = []
            clean_list_t = []            
            for h,t in self.hunting_dict.items():
                if t not in game_state.units[self.opp_player].keys():
                    clean_list_h.append(h)
                    clean_list_t.append(t)
            for t in clean_list_t:
                try:
                    self.opp_robots.pop(t)
                except:
                    print('DEBUG - unhandled error')
            for h in clean_list_h:
                self.hunting_dict.pop(h)
            def simple_manh_distance(src, target):
                ds=src-target
                return abs(ds[0])+abs(ds[1])
            
            hunting_pair_candidates = []
            for opp_unit_id, opp_unit in self.opp_robots.items():
                min_dist_to_target = 10000
                closest_unit_id = ''
                for unit_id, unit in units.items():
                    if (opp_unit['target'] == True) and (opp_unit['type'] == 'HEAVY') and (unit.power == 3000) and (unit_id not in self.hunting_dict.keys()) and (opp_unit_id not in self.hunting_dict.values()):
                        distance = simple_manh_distance(unit.pos, opp_unit['pos'])
                        if distance < min_dist_to_target:
                            min_dist_to_target = distance
                            closest_unit_id = unit_id
                if min_dist_to_target<=10:
                    hunting_pair_candidates.append({'opp_unit_id':opp_unit_id,'unit_id': closest_unit_id, 'distance': min_dist_to_target})
            
            if len(hunting_pair_candidates)>0:
                hunting_pair_candidates = pd.DataFrame(hunting_pair_candidates).sort_values('distance').drop_duplicates('unit_id', keep='first')
                for i, row in hunting_pair_candidates.iterrows():
                    self.hunting_dict[row['unit_id']]= row['opp_unit_id']
                    break
                
            dead_hunters=[]
            for hunter,target in self.hunting_dict.items():
                if hunter not in units.keys():
                    dead_hunters.append(hunter)
​
            for dh in dead_hunters:
                try:
                    self.hunting_dict.pop(dh)
                except:
                    pass
            for hunter,target in self.hunting_dict.items():        
                hunter_unit = units[hunter]
                direction = direction_to_mod_2(hunter_unit.pos, self.opp_robots[target]['pos'], opp_factories_map, step, 1000)
                if (type(direction) == list) and hunter not in self.my_robots_move_queue.keys():
                    self.my_robots_move_queue[hunter]=direction
                if hunter in self.my_robots_move_queue.keys():
                    direction = self.my_robots_move_queue[hunter][0]
                    move_cost = unit.move_cost(game_state, direction)
                    if move_cost is not None and hunter_unit.power >= move_cost + unit.action_queue_cost(game_state):
                        actions[hunter] = [hunter_unit.move(direction, repeat=0, n=1)]
                        self.my_robots_move_queue[hunter]=self.my_robots_move_queue[hunter][1:]
                        if len(self.my_robots_move_queue[hunter])==0:
                            self.my_robots_move_queue.pop(hunter)
                else:
                    move_cost = unit.move_cost(game_state, direction)
                    if move_cost is not None and hunter_unit.power >= move_cost + unit.action_queue_cost(game_state):
                        actions[hunter] = [hunter_unit.move(direction, repeat=0, n=1)]
                        #print('ah', actions[hunter])
        actions_before_coord = []
        for unit_id, unit in units.items():
            cur_pos = unit.pos
            if unit_id in actions.keys():
                cur_action = actions[unit_id]
                action_type = cur_action[0][0]
                if action_type == 0:
                    action_dir = cur_action[0][1]
                    pos_after = cur_pos + move_arrays[action_dir]                    
                else:
                    pos_after = cur_pos
            else:
                action_type = 99
                pos_after = cur_pos
            actions_before_coord.append({'unit_id':unit_id, 'action_type': action_type, 'pos_after':str(pos_after)})
        if len(actions_before_coord)>0:
            actions_before_coord = pd.DataFrame(actions_before_coord)
            actions_before_coord_wo_dupl = actions_before_coord.sort_values('action_type').drop_duplicates('pos_after', keep = 'last') 
            robots_to_wait = set(actions_before_coord['unit_id'].unique()).difference(actions_before_coord_wo_dupl['unit_id'].unique())
            for unit_id in robots_to_wait:
                actions.pop(unit_id)
        return actions
add Codeadd Markdown
sd = 18
add Codeadd Markdown
env_2 = gym.make("LuxAI_S2-v0")
env_2.reset(seed=sd)
img = env_2.render("rgb_array")
plt.imshow(img)
add Codeadd Markdown
# recreate our agents and run
agents = {player: Agent(player, env_2.state.env_cfg) for player in env_2.agents}
interact(env, agents, 500, sd)
add Codeadd Markdown
game_state = obs_to_game_state(25, env.state.env_cfg, env.observation_space('player_1'))
add Codeadd Markdown
game_state = obs_to_game_state(20, env.state.env_cfg, env)
